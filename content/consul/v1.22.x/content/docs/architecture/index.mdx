---
layout: docs
page_title: Consul architecture
description: >-
  Consul includes built-in security features and options to integrate existing security features to secure communication between users, agents in the control plane, and services in the data plane. 
# START AUTO GENERATED METADATA, DO NOT EDIT
created_at: 2026-01-16T14:22:25-08:00
last_modified: 2026-01-30T11:22:36-08:00
# END AUTO GENERATED METADATA
---

# Consul architecture

This page provides an overview of the architecture for Consul's operations.

## Introduction

Consul can support many different network topologies and cloud providers. The number of Consul agents and network components you deploy may change, but the underlying architecture of Consul's operations remain the same across systems, runtimes, and cloud environments.

![Diagram of the Consul control plane consensus traffic](/img/consul-arch/consul-arch-overview-control-plane.svg)

Consul operates in two areas to support your workloads:

- The _control plane_ is the part of Consul's infrastructure that maintains a central registry to track services and their respective IP addresses. It enables you to register, access, and secure services deployed across your network. To learn more, refer to [control plane architecture](/consul/docs/architecture/control-plane).
- The _data plane_ is the part of the network where workloads send application data to other nodes in the cluster. The Consul process does not run directly in the data plane, but when using Consul's service mesh features, you can deploy components such as sidecar proxies and gateways into the data plane to manage L4 and L7 network traffic between services. To learn more, refer to [data plane architecture](/consul/docs/architecture/data-plane).

## Backend data persistence

Consul servers in the control plane keep a record of the state of Consul agents and their registered services using the [Raft protocol for consensus](/consul/docs/concept/consensus). This protocol generates data in the form of a _Raft index_ that the Consul cluster must persist across reboots during cluster operations.

Consul logs the Raft index with the write-ahead log (WAL) LogStore backend. The WAL backend implements a traditional log with rotating, append-only log files, and it retains logs without affecting a cluster's write performance at scale.

Consul agents store this data in the folder specified with the [`-data-dir` command](/consul/commands/agent#_data_dir).

For more information about the Raft index and Consul's backend requirements, refer to [Persistent data backend architecture](/consul/docs/architecture/backend).

## Network tomography

Consul uses a network tomography system to compute _network coordinates_ for nodes in the cluster. These coordinates allow the network round trip time (RTT) to be estimated between any two nodes. Consul can use RTT when returning services. For example, it can find the service node that is nearest to the one making the request, or it can fail over to services in the next
closest datacenter.

Consul uses the the [Serf library](https://github.com/hashicorp/serf/), which also provides [gossip communication](/consul/docs/concept/gossip) between agents in a cluster, to determine network coordinates and their RTT.

For more information, refer to [network coordinates](/consul/docs/architecture/coordinates).

## Security architecture

Consul is secured by several methods for encrypted communication. These methods apply to Consul's operations in both the control plane and the data plane. For more information about how Consul secures communication across its architectural components, refer to [security architecture](/consul/docs/architecture/security).