---
page_title: Automate cloud storage lifecycle policies
description: Learn how to automate data lifecycle policies using Terraform and infrastructure as code. Reduce cloud storage costs, ensure compliance, and manage AWS S3, GCP, and Azure data retention policies.
# START AUTO GENERATED METADATA, DO NOT EDIT
created_at: 2025-09-10T10:35:54-05:00
last_modified: 2026-01-29T10:31:53-06:00
# END AUTO GENERATED METADATA
---

# Automate cloud storage lifecycle policies

Data lifecycle management policies help organizations automatically manage cloud storage costs, meet compliance requirements, and secure sensitive data. Using infrastructure as code tools like Terraform, you can define, version, and apply lifecycle rules across AWS S3, Google Cloud Storage, and Azure Blob Storage.

## Benefits of automated data lifecycle policies

Most major cloud providers offer lifecycle management features for their storage services. These features allow you to define rules that automatically transition data between different storage classes based on age or access patterns, and delete data that has reached the end of its retention period. 

When you implement data management policies, you gain the following benefits:
- Reduce storage costs by automatically deleting data that is no longer needed.
- Reduce storage costs by storing data in the most cost-effective storage class based on access patterns and retention requirements.
- Ensure compliance with legal and regulatory requirements for data retention.
- Minimize security risks by removing sensitive data after a defined period of time.

## Automate policy management with infrastructure as code

You can use Terraform to define and manage lifecycle policies and implement those policies across your organization. You can create Terraform modules to create data management policies for different data types and compliance requirements. These modules can automatically apply appropriate lifecycle rules, storage class transitions, and deletion policies to new or existing storage resources.

The following Terraform configuration defines a [data lifecycle policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_lifecycle_configuration#specifying-a-filter-based-on-object-size) to move AWS S3 data to Glacier Instant Retrieval after 365 days:

```hcl
resource "aws_s3_bucket_lifecycle_configuration" "example" {
  bucket = aws_s3_bucket.bucket.id

  rule {
    id = "Allow small object transitions"

    filter {
      object_size_greater_than = 1
    }

    status = "Enabled"

    transition {
      days          = 365
      storage_class = "GLACIER_IR"
    }
  }
}
```

Terraform can also tag resources with appropriate retention metadata. These tags can include creation dates, data classifications, and retention periods.

For example, you can use the [`tag` block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_lifecycle_configuration#specifying-a-filter-based-on-an-object-tag) with AWS S3 to automatically apply tags to all resources created by Terraform. The S3 lifecycle rule specifies a filter based on a tag key and value. The rule then applies only to a subset of objects with the specific tag.

```hcl
resource "aws_s3_bucket_lifecycle_configuration" "example" {
  bucket = aws_s3_bucket.bucket.id

  rule {
    id = "rule-1"

    filter {
      tag {
        key   = "Name"
        value = "Staging"
      }
    }

    transition {
      days          = 30
      storage_class = "GLACIER"
    }

    status = "Enabled"
  }
}
```

Other cloud providers, such as [Google Cloud Platform](https://registry.terraform.io/providers/hashicorp/google/5.0.0/docs/resources/storage_bucket.html#example-usage---life-cycle-settings-for-storage-bucket-objects) and [Microsoft Azure](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_management_policy), offer similar lifecycle management features for their storage services. You can use Terraform to manage lifecycle policies across multiple cloud providers, ensuring consistent data management practices regardless of where your data resides.

## HashiCorp resources

- Search the [Terraform Registry](https://registry.terraform.io/browse/providers) for the [cloud](https://registry.terraform.io/browse/providers?category=public-cloud) or [database](https://registry.terraform.io/browse/providers?category=database) provider you use.
- Learn best practices for writing Terraform with the Terraform [style guide](/terraform/language/style).
- Start learning Terraform with the [Get started tutorials](/terraform/tutorials).

### External resources

- Cloud storage: [AWS](https://aws.amazon.com/products/storage/), [GCP](https://cloud.google.com/products/storage), and [Azure](https://azure.microsoft.com/en-us/products/category/storage)
- [Learn how to set the lifecycle configuration for a Google Cloud Bucket](https://cloud.google.com/storage/docs/samples/storage-create-lifecycle-setting-tf) with Terraform.
- AWS [Enforce data retention policies](https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_decomissioning_resources_data_retention.html)

## Next steps

In this section of Lifecycle management, you learned about implementing data management policies, including why you should use lifecycle policies and how to automate policy management with infrastructure as code. Implement data management policies is part of the [Optimize systems](/well-architected-framework/optimize-systems) pillar.

To learn more about infrastructure and resource management, refer to the following resources:

- [Automate infrastructure provisioning](/well-architected-framework/define-and-automate-processes/process-automation/process-automation-workflow)
- [Tag cloud resources](/well-architected-framework/define-and-automate-processes/infrastructure-and-resource-management/tag-cloud-resources)
- [Decommission infrastructure resources](/well-architected-framework/optimize-systems/lifecycle-management/decommission-infrastructure)
