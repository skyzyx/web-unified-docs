---
page_title: Migrating from Replicated - Installation - Flexible Deployment Options Beta - Terraform Enterprise
description: >-
  Migrate from Terraform Enterprise Replicated to Terraform Enterprise Flexible Deployment Options.
# START AUTO GENERATED METADATA, DO NOT EDIT
created_at: 2023-01-01T00:00:00.000Z
last_modified: 2023-01-01T00:00:00.000Z
# END AUTO GENERATED METADATA
---

<Note title="Beta Documentation">
  This documentation supports the Beta release of Terraform Enterprise Flexible Deployment Options. The information here does not apply to other generally available releases of Terraform Enterprise. 
<br/>

  If you want to participate in the beta program, get in touch with your HashiCorp account team. For beta support, visit our [Support page](/terraform/enterprise/flexible-deployments-beta/admin/support) or [submit a request directly](https://support.hashicorp.com).
</Note>

# Migrating from Replicated

We support three migration paths from Replicated releases of Terraform Enterprise to the Flexible Deployment Options Beta.

1. [Migrating using mounted disk operational mode to Docker runtime](#mounted-disk-migrating-to-docker)
2. [Migrating using external services operational mode to Docker runtime](#external-services-migrating-to-docker) (Coming Soon!)
3. [Cloud-managed Kubernetes](#cloud-managed-kubernetes)

## Overview

At a high level, migrating from a Replicated release of Terraform Enterprise to Docker or managed Kubernetes follows the same pattern:
1. Back up your current Terraform Enterprise data tier (such as Postgres or Object Storage).
1. Upgrade your current Terraform Enterprise (Replicated) installation to the minimum compatible Terraform Enterprise version (v202307-1 for this beta).
1. Prepare your new installation of Terraform Enterprise (Docker or cloud-managed Kubernetes).
1. Migrate your data tier to your new installation.

## Mounted disk migrating to Docker

At a high level, complete the following steps to migrate a Terraform Enterprise installation that uses the mounted disk operational mode to Docker:

1. Back up your mounted disk and Terraform Enterprise configuration.
1. Upgrade your current Terraform Enterprise (Replicated) installation to the minimum compatible version for Flexible Deployment Options (v202307-1).
1. Ensure prerequisites are present on your current Terraform Enterprise (Replicated) host (such as `docker-compose`, `pull docker image`)
1. Prepare your `docker-compose` file and place it on the host in its own directory (`/etc/terraform-enterprise`), then create a `systemd` service for Terraform Enterprise.
1. Stop Terraform Enterprise (Replicated).
1. Start your new Docker compose-based Terraform Enterprise using the same mounted disk directory.
1. Conduct validation testing after the migration is complete.

If at any point you need to revert your settings, see [the rollback steps](/terraform/enterprise/flexible-deployments-beta/replicated-migration#rollback-steps).

### Prerequisites

Before proceeding with the migration guide, you need:
- An existing Terraform Enterprise (Replicated) installation running version v202307-1
- A Docker-compose based version of Terraform Enterprise's v202307-1 release
- A new Terraform Enterprise docker compose file

### Migration steps

You can complete parts 1 through 3 below in advance of a production migration date. If you do complete parts 1 through 3, ensure create a data tier backup before resuming part 4.

#### Part 1: Back up your data tier

We always recommend backing up your data tier before conducting maintenance. Learn more about our [recommended pattern for backing up your mounted disk](/terraform/tutorials/recommended-patterns/pattern-backups#backup-a-mounted-disk-deployment).

#### Part 2: Upgrade your existing TFE installation

Next, upgrade your existing Terraform Enterprise (Replicated) installation to the minimum version required for Flexible Deployment Options (v202307-1). This ensures your application data is compatible with the new Docker-based installation.

You must also enable consolidated services mode, which mirrors the architecture of your new installation. This allows you to validate that the new architecture is working properly before proceeding with additional steps.

1. Upgrade Terraform Enterprise on Replicated to at least [v202307-1](/terraform/enterprise/releases/2023/v202307-1).
1. From a shell on your Terraform Enterprise instance, enable `consolidated_services` mode.
   ```shell-session
   $ replicatedctl app-config set consolidated_services --value=1
   ```
1. Restart the Terraform Enterprise application.
   ```shell-session
   $ replicatedctl app stop
   ```

   Wait for the application to stop:
   ```shell-session
   $ replicatedctl app status
   ```

   Start the application:
   ```shell-session
   $ replicatedctl app start
   ```
1. The above commands condensed the number of containers you have. The most notable container, the `terraform-enterprise` container, contains a consolidated log stream of all services inside that container.
1. Validate that Terraform Enterprise upgraded successfully as you usually would (by creating and applying plans).

#### Part 3: Prepare the host and install your new Terraform Enterprise

To make migration smoother and faster, we recommend using the same host as your current Replicated instance.

Because Replicated is installed on this host, Docker should already be installed for you. However, you may still need to install Docker Compose if you have not already.

1. Create a directory called `/etc/terraform-enterprise` on the host for your `docker-compose.yml`. [Refer to our example for guidance](https://gist.githubusercontent.com/bnferguson/a5dd3ca9b0f7136b2c68f14b56770697/raw/c2152434d81b948be5adddd754f6faafff954964/docker-compose.yml), paying close attention to how we use bind mounts in the volumes section.
1. Place a `docker-compose.yml` file in your new directory and adjust it for your host and environment. Ensure that you update and verify the values below.
   * Hostname
   * Encryption Password
   * Certificates and their locations
   * Custom CA Bundle
   * Mounted disk data path
   * Proxy and no_proxy settings
1. Verify that your host has the login credentials for pulling the image, then pull the image.
   ```shell-session
   $ docker login quay.io
   $ docker pull quay.io/hashicorp/terraform-enterprise:v202307-1
   ```
1. Create a new systemd service for Terraform Enterprise by creating a file named, `/etc/systemd/system/terraform-enterprise.service` with [these contents](https://gist.githubusercontent.com/bnferguson/a5dd3ca9b0f7136b2c68f14b56770697/raw/c2152434d81b948be5adddd754f6faafff954964/docker-compose.yml). If you changed the name of the directory holding your `docker-compose.yml`, update the systemd service file path to match.

-> **Note**: If you want to use a separate host for your new docker-based Terraform Enterprise, we can provide alternative steps. [Reach out to support for assistance](/terraform/enterprise/flexible-deployments-beta/admin/support).

#### Part 4: Stop Replicated and migrate 

Next, you can stop Replicated and migrate your Terraform Enterprise installation to Docker.

1. SSH into your Terraform Enterprise (Replicated) instance.
1. Stop Terraform Enterprise (Replicated). Ensure the application has fully stopped before proceeding.
   ```shell-session
   $ replicatedctl app stop
   ```
1. Back up your data. If you have already backed up your data proceed forward. If not, or if you want another backup for safe keeping, do the following.
   * Retrieve your mounted disk path.
   ```shell-session
   $ replicatedctl app-config export --template '{{ .disk_path.Value }}' | tr -d '\r'
   ```
   * We will refer to this path as the `${DISK_PATH}` going forward. Next, archive your mounted disk data.
   ```shell-session
   $ tar -zcvf data.tar.gz -C ${DISK_PATH} aux postgres
   ```
   * Copy your `data.tar.gz` archive to a safe place.
1. Start (and enable on start up) the Docker Compose based Terraform Enterprise.
   ```shell-session
   $ systemctl enable --now terraform-enterprise
   ```
1. Check the status of your service with `systemctl status terraform-enterprise`, or use `docker ps` to find your container's name then run `docker logs [name]`. You can also run `curl https://[hostname]/_health_check` to check the health check endpoint. Terraform Enterprise should now be running using Docker Compose, so your Replicated services can be shut down and disabled. 

1. Shut down Replicated.
   ```shell-session
   $ systemctl disable --now replicated replicated-ui replicated-operator
   ```

1. Next, clean up the extra unneccessary Replicated containers. 
   ```shell-session
   $ docker stop replicated-premkit
   $ docker stop replicated-statsd
   $ docker rm -f replicated replicated-ui replicated-operator replicated-premkit replicated-statsd retraced-api retraced-processor retraced-cron retraced-nsqd retraced-postgres
   ```

Some of the `docker stop` commands may return “Container not found” errors because not every Replicated install has every container.

#### Part 5: Validate migration success

Finally, test that your new Terraform Enterprise installation works properly. If you have an existing suite of release acceptance tests, you can use those instead of doing the following steps.

1. Execute a plan and apply it from the CLI, testing several subsystems. Ensuring that proxies are correctly configured, certificates are properly configured, and the instance can download Terraform binaries and execute runs.
1. Execute a plan and apply it from VCS, testing that webhooks are working and certificates are in place on both sides.
1. Publish a new module to the private module registry.
1. Execute a plan and apply it with a module or provider from the private registry to ensure the registry is functioning. 
1. (_Optional_) Execute a plan and apply it with Sentinel and cost estimation, ensuring run tasks and cost estimation work.
1. (_Optional_) Execute a plan and apply it on a workspace that uses an agent pool, testing that external agents can connect and run jobs successfully.

#### Rollback steps

In the unlikely event you encounter issues and need to roll back, you can revert back to Terraform Enterprise (Replicated) using the following commands.

1. Stop and disable Terraform Enterprise (Docker).
   ```shell-session
   $ systemctl disable --now terraform-enterprise
   ```
1. Start and enable Replicated.
   ```shell-session
   $ systemctl enable --now replicated replicated-ui replicated-operator
   ```
1. Start Terraform Enterprise (Replicated)
   ```shell-session
   $ replicatectl app start
   ```

## External services migrating to Docker
The migration guide for migrating a Terraform Enterprise instance using the external services operational mode is coming soon!

## Cloud-managed Kubernetes

Before starting the process, you must upgrade your Terraform Enterprise installation to the minimum compatible version for Flexible Deployment Options (v202307-1).

If you are already running Replicated on that version or newer,
feel free to skip to _Part 1_ to whichever migration guide best fits your installation's operational mode configuration:
* [Migrating using mounted disk operational mode to Kubernetes](#mounted-disk-migrating-to-kubernetes) 
* [Migrating using external services operational mode to Kubernetes](#external-services-migrating-to-kubernetes) 

#### Prerequisites

Before starting, you must upgrade your existing Terraform Enterprise (Replicated) installation to the minimum version required for Flexible Deployment Options (v202307-1). 

You must also enable consolidated services mode, which mirrors the architecture of your new installation. This allows you to validate that the new architecture is working properly before proceeding with additional steps.

1. Follow your usual Terraform Enterprise upgrade process on Replicated, upgrading to at least [v202307-1](/terraform/enterprise/releases/2023/v202307-1).
1. From a shell on your Terraform Enterprise instance, enable `consolidated_services` mode.
   ```shell-session
   replicatedctl app-config set consolidated_services --value=1
   ```
1. Restart the Terraform Enterprise application.
   ```shell-session
   $ replicatedctl app stop
   ```

   Wait for the application to stop:
   ```shell-session
   $ replicatedctl app status
   ```

   Start the application:
   ```shell-session
   $ replicatedctl app start
   ```
1. The above commands condensed the number of containers you have. The most notable container, the `terraform-enterprise` container, contains a consolidated log stream of all services inside that container.
1. Validate that Terraform Enterprise upgraded successfully as you usually would (by creating and applying plans).

### Mounted disk migrating to Kubernetes

Follow the prerequisites for [installing Terraform Enterprise on Kubernetes](/terraform/enterprise/flexible-deployments-beta/install/requirements/kubernetes) before continuing.

~> **Note**: The only operational mode available for Kubernetes is `active-active`. Due to this restriction, you must provide an external Redis server.

If can configure your existing mounted disk installation to use an external database and object storage, we recommend [migrating from mounted disk to external services](/terraform/enterprise/operational-modes) before continuing this migration process. With external services, you can test your database configuration before migrating to Kubernetes and the rollback process is streamlined.

#### Part 1: Backup your data tier

Stop your mounted disk application, either via the Replicated UI, or by executing the following command.

```shell-session
$ replicatedctl app stop
```

Next, locate and navigate to the mounted disk path where your Terraform Enterprise data is stored. You can find this directory by running the following command.

```shell-session
$ replicatedctl app-config export --template '{{ .disk_path.Value }}'
```

Create a tar archive containing the "aux" and "postgres" directories.

```shell-session
$ tar -zcvf data.tar.gz aux postgres
```

Create a database dump using the `tfe-admin` utility. The following command interacts with the `tfe-admin` container to create this backup.

```shell-session
$ docker exec tfe-admin tfe-admin db-backup
```

Copy the dump file from your `tfe-postgres` container to the Docker host with the following command.

```shell-session
$ docker cp tfe-postgres:/backup/ptfe.db ptfe.db
```

You now have two files that you need to copy to your machine (or any source accessible to your Kubernetes installation services). To accomplish this using `scp`, use the following command with a few adjustments.

```shell-session
$ scp -r /local/directory username@tfe_host:/${disk_path}
```

Replace `local/directory` with the path to the local directory containing the files. Replace `username` with the appropriate username, and `tfe_host` with the host of the Terraform Enterprise mounted disk instance. Finally, replace `${disk_path}` with the
directory provided by the `replicatedctl` CLI.

#### Part 2: Restoring archivist files

Archivist is our internal tool for object storage management. This is where Terraform Enterprise stores `terraform.tfstate`, plans, applies, and other artifacts. The files are encrypted using Vault, and the `TFE_ENCRYPTION_PASSWORD` value is your keyword. Treat these files as _sensitive data_.

Make these files available to your Kubernetes installation by doing the following:

1. Unpackage the contents of the `data.tar.gz` archive that you previously copied to your machine.
1. Upload the contents from `aux/archivist` into your Kubernetes deployment's configured object storage system.
   - Use the `TFE_OBJECT_STORAGE_TYPE` and `TFE_OBJECT_STORAGE_*` variables to specify where all the credentials and container/bucket values are configured.

The key-directory structure of the bucket should start with the archivist directory. Here's an example of how the final state of the bucket should look:

```sh
├── archivist
   ├── terraform
      ├── json-plans
      ├── json-provider-schemas
      ├── logs
      ├── plans
      ├── slugs
      └── states
```

#### Part 3: Restoring the database

At this point, you must stop your Terraform Enterprise application in Kubernetes _without_ killing the pod. You can accomplish this using the `tfectl` CLI utility bundled inside the Terraform Enterprise image:

1. Connect to any pod's shell in your Kubernetes installation.
2. Run the following command.
   ```shell-session
   $ tfectl node drain
   ```
   - In the case of an `active-active` installation, drain all the nodes by running the following command.

   ```shell-session
   $ tfectl node drain --all
   ```

Next, connect to a shell session that can access your Kubernetes installation's PostgreSQL instance. Set up the session using the `TFE_DATABASE_HOST` environment variable. Copy the `ptfe.db` file from the backup section into a directory you can access.

Next, create a new database under the PostgreSQL instance where you can then restore all the contents of your mounted disk installation. Use the [createdb command](https://www.postgresql.org/docs/current/app-createdb.html) with the following parameters:

```shell-session
$ createdb -h ${TFE_DATABSE_HOST} -U tfe_user -T template0 "tfe_restore"
```

This command prompts for the password set up in the `TFE_DATABASE_PASSWORD` environment variable.

After setting up the database, restore the contents of the `ptfe.db` file into the `tfe_restore` database using [`pg_restore`](https://www.postgresql.org/docs/current/app-pgrestore.html):

```shell-session
$ pg_restore -x --no-owner --dbname=tfe_restore --exit-on-error -Fc ptfe.db --h ${TFE_DATABASE_HOST} -U tfe_user
```

This command also prompts for the `TFE_DATABASE_PASSWORD` value.

Now, it is time to switch from the `tfe_restore` database to the database you configured under the `TFE_DATABASE_NAME` environment variable. You can do this with a database rename operation. 

To ensure all connections to the `TFE_DATABASE_NAME` database are closed, execute the following SQL statement:

```sql
SELECT
   pg_terminate_backend (pid)
FROM
   pg_stat_activity
WHERE
   datname = '${TFE_DATABASE_NAME}';
```

It is important that you do this from the `tfe_restore` database, because is not possible to rename the database that you are connected to. To proceed with the rename operation, use the following SQL statement:

```sql
ALTER DATABASE ${TFE_DATABASE_NAME} RENAME TO ${TFE_DATABASE_NAME}_backup;
```

Once the rename is complete, connect to the `${TFE_DATABASE_NAME}_backup` database to execute the rename operation for the `tfe_restore` database. Start by closing all connections using the previous SQL statement. Then, rename the database using the following SQL statement:

```sql
ALTER DATABASE tfe_restore RENAME TO ${TFE_DATABASE_NAME};
```

After confirming the restoration without any missing data, remove the `${TFE_DATABASE_NAME}_backup` database.

#### Part 4: Start Terraform Enterprise in Kubernetes

~> **Note:** It is crucial that you configure your new Kubernetes installation with the same encryption password as your previous Replicated installation.

To retrieve this password value, connect to your instance via SSH and run the following command.

```shell-session
$ replicatedctl app-config export --template '{{ .enc_password.Value }}'
```

During the helm chart installation, you can provide this value through the `values.yml` override for the environment variables.

```yaml
env:
   secrets:
      TFE_ENCRYPTION_PASSWORD: ${REPLICATED_ENCRYPTION_PASSWORD}
```

Ensure you replace `${REPLICATED_ENCRYPTION_PASSWORD}` with the actual value obtained from the `replicatedctl` command.

#### Rollback steps

In the unlikely event you encounter issues and need to roll back, you can revert back to Terraform Enterprise (Replicated) using the following commands.

If you changed your mounted disk installation to use external services mode, you must modify your database and Redis connection strings to use the instances running inside the Terraform Enterprise container, rather than the ones enabling the external services operation.

All your pods running in Kubernetes can be safely shut down.

If you are migrating directly from the mounted disk container to Kubernetes, your original installation is intact. You can safely shut down your Kubernetes deployment and [refer to support](/terraform/enterprise/flexible-deployments-beta/admin/support) for further help.

### External services migrating to Kubernetes

#### Part 1: Backup your data tier

We always recommend backing up your data tier before conducting maintenance.

#### Part 2: Prepare a new Terraform Enterprise Helm chart
The prerequisites for installing Terraform Enterprise on Kubernetes are in the [Kubernetes installation guide](/terraform/enterprise/flexible-deployments-beta/install/requirements/kubernetes).

~> **Note**: The only operational mode available for Kubernetes is `active-active`. Due to this restriction, you need to provide an external Redis server.

Use the external services credentials from your Replicated installation for the Kubernetes installation. Specifically the following:
1. The `TFE_OBJECT_STORAGE_TYPE` and `TFE_OBJECT_STORAGE_*` variables should specify the object storage type and the container or bucket credentials from your Replicated installation.
1. The `TFE_DATABASE_*` variables should specify database credentials from the Replicated installation.
1. If you use an external Redis for your Replicated installation, you can use the same credentials for the Kubernetes installation by replacing the `TFE_REDIS_*` values on the Helm chart.
1. If you use an external Vault on your Replicated installation, replace the `TFE_VAULT_*` values on the chart with the Replicated values.
1. Your `TFE_ENCRYPTION_PASSWORD` value should match your Replicated installation value. You can get this from your Replicated instance via SSH by running the following command:
   ```shell-session
   $ replicatedctl app-config export --template '{{ .enc_password.Value }}'
   ```

#### Part 3: Migrate data tier to Kubernetes
1. Stop your Replicated installation by executing the following command:

   ```shell-session
   $ replicatedctl app stop
   ```

1. Wait for the application to stop:
   ```shell-session
   $ replicatedctl app status
   ```

1. [Install Terraform Enterprise on Kubernetes using Helm](/terraform/enterprise/flexible-deployments-beta/install/kubernetes).
1. After installation, log in to your new Terraform Enterprise installation and ensure all your previous data remains.

Sign in with the credentials you previously used for your Replicated installation, and you should see the same workspaces and runs from your previous installation.

#### Rollback steps
In the unlikely event you encounter issues that cannot be worked around, you can rollback to Terraform Enterprise (Replicated).

1. If it is possible to `exec` into the pods, run the `node drain` command to stop Terraform Enterprise from executing further instructions.
   ```shell-session
   $ tfectl node drain --all
   ```

1. Uninstall the deployment.
   ```shell-session
   $ helm uninstall terraform-enterprise
   ```

1. Restart Terraform Enterprise on Replicated using the same external services.
   ```shell-session
   $ replicatedctl app start
   ```

## Troubleshooting

### Self signed certificates CA not in CA bundle
#### Symptoms:
 - Plans Fail
 - See errors in `/var/log/terraform-enterprise/task-worker.log` and `/var/log/terraform-enterprise/atlas.log` (particularly when making calls to Archivist) where the certificate is from an unknown issuer
#### Fix:
 - Bring the additional certificates from the full chain certificate into the CA Bundle
 - Note: Replicated did some of this automatically, for TFE FDO at this time you may need to manually concatenate the certificates from the full chain certificate into the CA Bundle for the instance to talk to itself.

### Required CA not in CA bundle
#### Symptom:
 - Setting up VCS fails with unknown certificate issuer error
#### Fix:
 - Include the CA in the CA Bundle

### Internal calls to instance or to AWS Metadata Endpoint getting proxied when they shouldn’t
#### Symptom:
 - Plans May fail, Logs may fail to load (but not always)
 - Traffic that shouldn’t be proxied, is
#### Fix:
 - Under Replicated we would build much of the default no_proxy/NO_PROXY address list. At the moment this is completely managed by the customer. This means in addition to what was once in their “Additional No Proxy List” some extra entries need to be added:
   `localhost,127.0.0.1,169.254.169.254,[FQDN of instance],[Rest of no_proxy list]`
